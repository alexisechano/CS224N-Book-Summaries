{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic GPT-2 Model\n",
    "\n",
    "We are now using HuggingFace's model! I am currently using [this article](https://www.modeldifferently.com/en/2021/12/generaci%C3%B3n-de-fake-news-con-gpt-2/) and [this HuggingFace link](https://huggingface.co/gpt2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hugging face\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline, set_seed\n",
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer, AdamWeightDecay, TextGenerationPipeline\n",
    "from transformers import DefaultDataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic GPT-2 model (from the site)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "text = \"This is a comedy story:\"\n",
    "text_ids = tokenizer.encode(text, return_tensors = 'pt')\n",
    "\n",
    "generated_text_samples = model.generate(text_ids)\n",
    "\n",
    "#Print output for each sequence generated above\n",
    "for i, beam in enumerate(generated_text_samples):\n",
    "  print(\"{}: {}\".format(i,tokenizer.decode(beam, skip_special_tokens=True)))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use hugging face documentation\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(text, max_length=50, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create smaller dataset from our subset data\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "filename = 'data/500_books.txt'\n",
    "df = pd.read_csv(filename, sep=\"\\t\", \n",
    "                 names=['Wikipedia ID', 'Freebase ID', 'Title', 'Author', 'Publication Date', 'Genres', 'Summary'])\n",
    "\n",
    "# clean data\n",
    "import re\n",
    "\n",
    "def clean(text):\n",
    "    cleaned_text = \"\"\n",
    "    punc_less_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    alpha_only_text = re.sub(r'[^a-zA-Z]',' ',punc_less_text)\n",
    "    cleaned_text = ' '.join(alpha_only_text.split())\n",
    "    return cleaned_text.lower()\n",
    "\n",
    "# apply to dataframe col that contains the book summary\n",
    "df['CleanSummary'] = df['Summary'].apply(lambda s: clean(s))\n",
    "df.head(5)\n",
    "\n",
    "# remove stop words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# download stopwords list\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "  stop_less = ' '.join([word for word in text.split() if word not in (stop_words)])\n",
    "  return stop_less\n",
    "\n",
    "# apply stopword removal to dataframe col that contains the book summary\n",
    "df['CleanSummary'] = df['CleanSummary'].apply(lambda s: remove_stopwords(s))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# drop data\n",
    "df = df.drop_duplicates(subset=['Wikipedia ID'])\n",
    "df = df.dropna(subset=['Genres','CleanSummary', 'Summary'])\n",
    "df['Genres'] = df['Genres'].map(lambda genre : list(json.loads(str(genre)).values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create condensed data with tokens\n",
    "BOS_TOKEN = '<BOS> '\n",
    "EOS_TOKEN = ' <EOS>'\n",
    "SPECIAL_TOKENS = []\n",
    "\n",
    "def transform_genres(genre_list):\n",
    "    genre_token = ''\n",
    "    for genre in genre_list:\n",
    "        genre_token += ('<' + genre + '>')\n",
    "        if genre_token not in SPECIAL_TOKENS:\n",
    "            SPECIAL_TOKENS.append(genre)\n",
    "        genre_token += ' '\n",
    "    return genre_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go thru data and clean up\n",
    "new_data = []\n",
    "df = df.reset_index()  # make sure indexes pair with number of rows\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    stringified_row = BOS_TOKEN + transform_genres(row['Genres']) + row['Summary'] + EOS_TOKEN\n",
    "    new_data.append(stringified_row)\n",
    "\n",
    "print(new_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe\n",
    "tokens_df = pd.DataFrame(new_data, columns=['Text'])\n",
    "tokens_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the data into training and test data: 80:20\n",
    "train_data, test_data = train_test_split(tokens_df, test_size=.2, random_state=8)\n",
    "\n",
    "# create HuggingFace Dataset\n",
    "train_ds = Dataset.from_pandas(train_data, split=\"train\")\n",
    "test_ds = Dataset.from_pandas(test_data, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize datasets\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"Text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_tok_ds = train_ds.map(tokenize_function, batched=True).shuffle(seed=42).select(range(50))\n",
    "test_tok_ds = test_ds.map(tokenize_function, batched=True).shuffle(seed=42).select(range(50))\n",
    "\n",
    "special_tokens_dict = {\n",
    "    \"bos_token\": BOS_TOKEN,\n",
    "    \"eos_token\": EOS_TOKEN,\n",
    "    \"pad_token\": \"<PAD>\",\n",
    "    \"additional_special_tokens\": SPECIAL_TOKENS,\n",
    "}\n",
    "\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import evals\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=\"test_trainer\",\n",
    "        overwrite_output_dir=True,\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        evaluation_strategy='no',\n",
    "        per_device_train_batch_size=4,\n",
    "        num_train_epochs=1,\n",
    "        save_total_limit=1,\n",
    "        save_steps=1000)\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_tok_ds,\n",
    "        eval_dataset=test_tok_ds,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"test_trainer\"\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "story_generator = TextGenerationPipeline(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = \"<BOS> <horror>\"\n",
    "story = story_generator(input_prompt, max_length=75, do_sample=True,\n",
    "               repetition_penalty=1.1, temperature=1.2, \n",
    "               top_p=0.95, top_k=50)\n",
    "print(story)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
