{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXiBy1yCv3BJ"
      },
      "source": [
        "# GPT-2 Model\n",
        "\n",
        "We are using HuggingFace's model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHU1A1QMv3BM"
      },
      "outputs": [],
      "source": [
        "# uncomment for colab\n",
        "%pip install transformers datasets accelerate nvidia-ml-py3 evaluate\n",
        "\n",
        "# import hugging face\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFenEXY7SBCW"
      },
      "outputs": [],
      "source": [
        "# for colab to keep track of utilization\n",
        "\n",
        "from pynvml import *\n",
        "\n",
        "def print_gpu_utilization():\n",
        "    nvmlInit()\n",
        "    handle = nvmlDeviceGetHandleByIndex(0)\n",
        "    info = nvmlDeviceGetMemoryInfo(handle)\n",
        "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
        "\n",
        "\n",
        "def print_summary(result):\n",
        "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
        "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
        "    print_gpu_utilization()\n",
        "\n",
        "# print GPU utilization\n",
        "print_gpu_utilization()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uhFnVzxv3BP"
      },
      "outputs": [],
      "source": [
        "# create smaller dataset from our subset data\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "filename = 'data/5000_booksummaries.zip'\n",
        "tokens_df = pd.read_csv(filename)\n",
        "tokens_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4gKsSoLv3BR"
      },
      "outputs": [],
      "source": [
        "# split data into train and test/eval data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split into train (80%), val (10%), test (10%)\n",
        "train_data, test_eval_dataset = train_test_split(tokens_df, test_size=0.2, random_state=8)\n",
        "eval_set, test_set = train_test_split(test_eval_dataset, test_size=0.5, random_state=8)\n",
        "\n",
        "# create HuggingFace Datasets\n",
        "train_ds = Dataset.from_pandas(train_data)\n",
        "eval_ds = Dataset.from_pandas(eval_set)\n",
        "test_ds = Dataset.from_pandas(test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qc8C9IULv3BR"
      },
      "outputs": [],
      "source": [
        "# tokenize datasets\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"Text\"], truncation=True)\n",
        "\n",
        "train_tok_ds = train_ds.map(tokenize_function, batched=True).shuffle(seed=42)\n",
        "eval_tok_ds = eval_ds.map(tokenize_function, batched=True).shuffle(seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqiGNMuwv3BS"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "epochs = 3 # change between 3 and 5\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "        output_dir=\"temp_model\",\n",
        "        overwrite_output_dir=True,\n",
        "        do_train=True,\n",
        "        do_eval=True,\n",
        "        evaluation_strategy='epoch',\n",
        "        per_device_train_batch_size=4,\n",
        "        num_train_epochs=epochs,   \n",
        "        save_total_limit=1,\n",
        "        logging_steps=250,\n",
        "        gradient_accumulation_steps=4, \n",
        "        gradient_checkpointing=True, \n",
        "        fp16=True\n",
        ") \n",
        "\n",
        "trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=train_tok_ds,\n",
        "        eval_dataset=eval_tok_ds,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCaxbjZnv3BS"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKwwPbY1v3BS"
      },
      "outputs": [],
      "source": [
        "# save local version\n",
        "checkpoint = \"./model_config\"\n",
        "model.save_pretrained(checkpoint)\n",
        "tokenizer.save_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cl63WDCh1Fj0"
      },
      "outputs": [],
      "source": [
        "%zip -r /content/model_setup_5000_3.zip /content/model_config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "619s82fS-Ugc"
      },
      "source": [
        "# Testing the Model\n",
        "\n",
        "Now, we can utilize our trained model parameters to conduct analysis and test on our dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccZMesCpOlPg"
      },
      "outputs": [],
      "source": [
        "# load into model and tokenizer\n",
        "model = GPT2LMHeadModel.from_pretrained(checkpoint)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzsKvlUov3BS"
      },
      "outputs": [],
      "source": [
        "# load input prompt\n",
        "input_prompt = \"Generate a book summary with genre science fiction:\\n\"\n",
        "inputs = tokenizer(input_prompt, return_tensors=\"pt\")\n",
        "\n",
        "# generate output from pretrained experiments (see baseline file)\n",
        "outputs = model.generate(**inputs, \n",
        "    max_length=150, \n",
        "    num_beams=2, \n",
        "    no_repeat_ngram_size=2, \n",
        "    do_sample=True,\n",
        "    early_stopping=True)\n",
        "\n",
        "# decode output and print out summary\n",
        "output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "print(output[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "cs224n",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.15"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
