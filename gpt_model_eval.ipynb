{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgHYz8hjYUmY"
      },
      "source": [
        "# Evaluation of GPT-2 Models\n",
        "\n",
        "In the first half of this notebook, we used qualitative text generation while the second half is assessing the quantitative metrics of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTNqzs7JYyvt"
      },
      "outputs": [],
      "source": [
        "%pip install transformers datasets accelerate evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jq7QARpUYUmb"
      },
      "outputs": [],
      "source": [
        "# get the file data\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfb4YYlnYk2k"
      },
      "outputs": [],
      "source": [
        "# mount the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6BB85cQYpMb"
      },
      "outputs": [],
      "source": [
        "# unzip model config files (google drive only) -> 3 or 5 epochs\n",
        "%unzip /content/drive/MyDrive/GPTModels/model_setup_5000_3.zip -d /content/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMs0SNNYYUmc"
      },
      "outputs": [],
      "source": [
        "# read in data\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "# google drive version\n",
        "filename = '/content/drive/MyDrive/GPTModels/5000_booksummaries.zip' #data/5000_booksummaries.zip'\n",
        "tokens_df = pd.read_csv(filename)\n",
        "tokens_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3pWLGrTYUmd"
      },
      "outputs": [],
      "source": [
        "# split data into train and test/eval data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split into train (80%), val (10%), test (10%)\n",
        "train_data, test_eval_dataset = train_test_split(tokens_df, test_size=0.2, random_state=8)\n",
        "eval_set, test_set = train_test_split(test_eval_dataset, test_size=0.5, random_state=8)\n",
        "\n",
        "# create HuggingFace Datasets\n",
        "train_ds = Dataset.from_pandas(train_data)\n",
        "eval_ds = Dataset.from_pandas(eval_set)\n",
        "test_ds = Dataset.from_pandas(test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYyB1XRyYUme"
      },
      "outputs": [],
      "source": [
        "# finetuned model\n",
        "checkpoint = '/content/models/content/model_config'\n",
        "model = GPT2LMHeadModel.from_pretrained(checkpoint)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "\"\"\"\n",
        "# vanilla model\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-qyKUZgYUme"
      },
      "outputs": [],
      "source": [
        "# THIS JUST GENERATES ONE OUTPUT!\n",
        "\n",
        "# load input prompt\n",
        "input_prompt = \"Generate a book summary with genre novel:\\n\"\n",
        "inputs = tokenizer(input_prompt, return_tensors=\"pt\")\n",
        "\n",
        "# generate output from pretrained experiments (see baseline file)\n",
        "outputs = model.generate(**inputs, \n",
        "    max_length=150, \n",
        "    num_beams=2, \n",
        "    no_repeat_ngram_size=2, \n",
        "    do_sample=True,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "# decode output and print out summary\n",
        "output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "print(output[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ene0c9slR9ZU"
      },
      "outputs": [],
      "source": [
        "# load input prompt\n",
        "input_prompt = \"Generate a book summary with genre science fiction, speculative fiction:\\n\"\n",
        "inputs = tokenizer(input_prompt, return_tensors=\"pt\")\n",
        "\n",
        "# generate output from pretrained experiments (see baseline file)\n",
        "outputs = model.generate(**inputs, \n",
        "    max_length=150, \n",
        "    num_beams=2, \n",
        "    no_repeat_ngram_size=2, \n",
        "    do_sample=True,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "# decode output and print out summary\n",
        "output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "print(output[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNJ89fYESBUe"
      },
      "outputs": [],
      "source": [
        "# load input prompt\n",
        "input_prompt = \"Generate a book summary with genre children's literature:\\n\"\n",
        "inputs = tokenizer(input_prompt, return_tensors=\"pt\")\n",
        "\n",
        "# generate output from pretrained experiments (see baseline file)\n",
        "outputs = model.generate(**inputs, \n",
        "    max_length = 200,\n",
        "    num_beams=2, \n",
        "    no_repeat_ngram_size=2, \n",
        "    do_sample=True,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "# decode output and print out summary\n",
        "output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "print(output[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krbKe72h2l6M"
      },
      "source": [
        "# Quantitative Scores\n",
        "\n",
        "We used BERTScore and Perplexity/Loss calculations for this section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeybRSPsYUmf"
      },
      "outputs": [],
      "source": [
        "# use BERTScores to analyze\n",
        "%pip install bert_score\n",
        "from evaluate import load\n",
        "bertscore = load(\"bertscore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyfWBXRwYUmf"
      },
      "outputs": [],
      "source": [
        "# helper functions\n",
        "def truncate_to_prompt(whole_text):\n",
        "    tok = whole_text.index(':')\n",
        "    return whole_text[:tok+2] # returns text with new line\n",
        "\n",
        "def generate_summary_from_prompt(prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # generate output from pretrained experiments , just comment out params from num_beans to end if no good decoding\n",
        "    outputs = model.generate(**inputs, max_length=150, num_beams=2, no_repeat_ngram_size=2, do_sample=True,early_stopping=True)\n",
        "\n",
        "    # decode output and return out summary\n",
        "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atSK8BPxYUmf"
      },
      "outputs": [],
      "source": [
        "# run model to generate predictions\n",
        "references = []\n",
        "predictions = []\n",
        "truncated_test_inputs = []\n",
        "\n",
        "for example in test_ds:\n",
        "    input = example[\"Text\"]\n",
        "    prompt_only = truncate_to_prompt(input)\n",
        "    truncated_test_inputs.append(prompt_only)\n",
        "    references.append(input)\n",
        "\n",
        "    # make predictions\n",
        "    predictions.append(generate_summary_from_prompt(prompt_only))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDBo2ribYUmg"
      },
      "outputs": [],
      "source": [
        "results = bertscore.compute(predictions=predictions, references=references, lang=\"en\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xddCaVTY2_Zk"
      },
      "outputs": [],
      "source": [
        "def avg(number_list):\n",
        "  return sum(number_list)/len(number_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVSNNbCVYUmg"
      },
      "outputs": [],
      "source": [
        "# print results and stats\n",
        "print(\"Raw Results\")\n",
        "print('PRECISION: ' + str(results['precision']))\n",
        "print('RECALL: ' + str(results['recall']))\n",
        "print('F1: ' + str(results['f1']))\n",
        "print()\n",
        "\n",
        "print(\"Averages\")\n",
        "print('PRECISION: ' + str(avg(results['precision'])))\n",
        "print('RECALL: ' + str(avg(results['recall'])))\n",
        "print('F1: '  + str(avg(results['f1'])))\n",
        "print()\n",
        "print(\"Max Values\")\n",
        "print('PRECISION: ' + str(max(results['precision'])))\n",
        "print('RECALL: ' + str(max(results['recall'])))\n",
        "print('F1: ' + str(max(results['f1'])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e43UlqYLq5Gd"
      },
      "outputs": [],
      "source": [
        "# calculating loss and perplexity\n",
        "from evaluate import load\n",
        "perplexity = load(\"perplexity\", module_type= \"measurement\")\n",
        "results = perplexity.compute(data=predictions, model_id=checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwjAPYObuUJ6"
      },
      "outputs": [],
      "source": [
        "print(str(avg(results['perplexities'])))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "cs224n",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.15"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
