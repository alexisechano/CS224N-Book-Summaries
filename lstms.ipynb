{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic LSTM's to classify genres\n",
    "In this notebook, we configure and train a basic Long Short-Term Memory Networks (LSTM's). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmc/cs224n/CS224N-Book-Summaries/cs224n_finalproj/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt   \n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12ce23030>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/Users/jmc/.cache/huggingface/datasets/csv/default-7ea8802e0a5d8de9/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|██████████| 1/1 [00:00<00:00, 193.05it/s]\n",
      "Found cached dataset csv (/Users/jmc/.cache/huggingface/datasets/csv/default-cced43b195c555d7/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|██████████| 1/1 [00:00<00:00, 301.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Unnamed: 0', 'Wikipedia ID', 'Freebase ID', 'Title', 'Author', 'Publication Date', 'Genres', 'Summary', 'CleanSummary'],\n",
      "        num_rows: 2519\n",
      "    })\n",
      "})\n",
      " In October 1921, Andrew Manson, an idealistic, newly qualified doctor, arrives from Scotland to work as assistant to Doctor Page in the small Welsh mining town of 'Drineffy'. He quickly realises that Page is an invalid and that he has to do all the work for a meagre wage. Shocked by the unsanitary conditions he finds, he works to improve matters and receives the support of Dr Philip Denny, a cynical semi-alcoholic. Resigning, he obtains a post as assistant in a miners' medical aid scheme in 'Aberalaw', a neighbouring coal mining town in the South Wales coalfield. On the strength of this job, he marries Christine Barlow, a junior school teacher. Christine helps her husband with his silicosis research. Eager to improve the lives of his patients, mainly coal miners, Manson dedicates many hours to research in his chosen field of lung disease. He studies for, and is granted, the MRCP, and when his research is published, an MD. The research gains him a post with the 'Mines Fatigue Board' in London, but he resigns after six months to set up a private practice. Seduced by the thought of easy money from wealthy clients rather than the principles he started out with, Manson becomes involved with pampered private patients and fashionable surgeons and drifts away from his wife. A patient dies because of a surgeon's ineptitude, and the incident causes Manson to abandon his practice and return to his former ways. He and his wife repair their damaged relationship, but Christine is killed in a traffic accident. It is Denny, now teetotal, who whisks him off to the Welsh countryside to recover. Since Manson had accused the incompetent surgeon of murder, he is vindictively reported to the General Medical Council for having worked with an American tuberculosis specialist who does not have a medical degree, even though the patient had been successfully treated at his nature cure clinic. Despite his lawyer's gloomy prognosis, Manson forcefully justifies his actions during the hearing and is not struck off the medical register. He joins Denny and bacteriologist Dr Hope in opening an integrated, multi-specialty practice, then uncommon, in a country town.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "data_files = {\"train\": \"data/train_data.zip\", \"test\": \"data/test_data.zip\"}\n",
    "train_dataset = load_dataset(\"csv\", data_files=data_files[\"train\"])\n",
    "test_dataset = load_dataset(\"csv\", data_files=data_files[\"test\"])\n",
    "print(test_dataset)\n",
    "print(test_dataset['train'][35]['Summary'])  # for some reason you still need to use ['train']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/jmc/.cache/huggingface/datasets/csv/default-7ea8802e0a5d8de9/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-544fd189ae847b82.arrow\n",
      "Loading cached processed dataset at /Users/jmc/.cache/huggingface/datasets/csv/default-cced43b195c555d7/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-7d5765d4a649be26.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Unnamed: 0', 'Wikipedia ID', 'Freebase ID', 'Title', 'Author', 'Publication Date', 'Genres', 'Summary', 'CleanSummary', 'tokens'],\n",
      "        num_rows: 2519\n",
      "    })\n",
      "})\n",
      "['in', 'october', '1921', ',', 'andrew', 'manson', ',', 'an', 'idealistic', ',', 'newly', 'qualified', 'doctor', ',', 'arrives', 'from', 'scotland', 'to', 'work', 'as', 'assistant', 'to', 'doctor', 'page', 'in', 'the', 'small', 'welsh', 'mining', 'town', 'of', \"'\", 'drineffy', \"'\", '.', 'he', 'quickly', 'realises', 'that', 'page', 'is', 'an', 'invalid', 'and', 'that', 'he', 'has', 'to', 'do', 'all', 'the', 'work', 'for', 'a', 'meagre', 'wage', '.', 'shocked', 'by', 'the', 'unsanitary', 'conditions', 'he', 'finds', ',', 'he', 'works', 'to', 'improve', 'matters', 'and', 'receives', 'the', 'support', 'of', 'dr', 'philip', 'denny', ',', 'a', 'cynical', 'semi-alcoholic', '.', 'resigning', ',', 'he', 'obtains', 'a', 'post', 'as', 'assistant', 'in', 'a', 'miners', \"'\", 'medical', 'aid', 'scheme', 'in', \"'\", 'aberalaw', \"'\", ',', 'a', 'neighbouring', 'coal', 'mining', 'town', 'in', 'the', 'south', 'wales', 'coalfield', '.', 'on', 'the', 'strength', 'of', 'this', 'job', ',', 'he', 'marries', 'christine', 'barlow', ',', 'a', 'junior', 'school', 'teacher', '.', 'christine', 'helps', 'her', 'husband', 'with', 'his', 'silicosis', 'research', '.', 'eager', 'to', 'improve', 'the', 'lives', 'of', 'his', 'patients', ',', 'mainly', 'coal', 'miners', ',', 'manson', 'dedicates', 'many', 'hours', 'to', 'research', 'in', 'his', 'chosen', 'field', 'of', 'lung', 'disease', '.', 'he', 'studies', 'for', ',', 'and', 'is', 'granted', ',', 'the', 'mrcp', ',', 'and', 'when', 'his', 'research', 'is', 'published', ',', 'an', 'md', '.', 'the', 'research', 'gains', 'him', 'a', 'post', 'with', 'the', \"'\", 'mines', 'fatigue', 'board', \"'\", 'in', 'london', ',', 'but', 'he', 'resigns', 'after', 'six', 'months', 'to', 'set', 'up', 'a', 'private', 'practice', '.', 'seduced', 'by', 'the', 'thought', 'of', 'easy', 'money', 'from', 'wealthy', 'clients', 'rather', 'than', 'the', 'principles', 'he', 'started', 'out', 'with', ',', 'manson', 'becomes', 'involved', 'with', 'pampered', 'private', 'patients', 'and', 'fashionable', 'surgeons', 'and', 'drifts', 'away', 'from', 'his', 'wife', '.', 'a', 'patient', 'dies', 'because', 'of', 'a', 'surgeon', \"'\", 's', 'ineptitude', ',', 'and', 'the', 'incident', 'causes', 'manson', 'to', 'abandon', 'his', 'practice', 'and', 'return', 'to', 'his', 'former', 'ways', '.', 'he', 'and', 'his', 'wife', 'repair', 'their', 'damaged', 'relationship', ',', 'but', 'christine', 'is', 'killed', 'in', 'a', 'traffic', 'accident', '.', 'it', 'is', 'denny', ',', 'now', 'teetotal', ',', 'who', 'whisks', 'him', 'off', 'to', 'the', 'welsh', 'countryside', 'to', 'recover', '.', 'since', 'manson', 'had', 'accused', 'the', 'incompetent', 'surgeon', 'of', 'murder', ',', 'he', 'is', 'vindictively', 'reported', 'to', 'the', 'general', 'medical', 'council', 'for', 'having', 'worked', 'with', 'an', 'american', 'tuberculosis', 'specialist', 'who', 'does', 'not', 'have', 'a', 'medical', 'degree', ',', 'even', 'though', 'the', 'patient', 'had', 'been', 'successfully', 'treated', 'at', 'his', 'nature', 'cure', 'clinic', '.', 'despite', 'his', 'lawyer', \"'\", 's', 'gloomy', 'prognosis', ',', 'manson', 'forcefully', 'justifies', 'his', 'actions', 'during', 'the', 'hearing', 'and', 'is', 'not', 'struck', 'off', 'the', 'medical', 'register', '.', 'he', 'joins', 'denny', 'and', 'bacteriologist', 'dr', 'hope', 'in', 'opening', 'an', 'integrated', ',', 'multi-specialty', 'practice', ',', 'then', 'uncommon', ',', 'in', 'a', 'country', 'town', '.']\n"
     ]
    }
   ],
   "source": [
    "# tokenize data, keep punctuation for EOS\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "tokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example['Summary'])}  \n",
    "tokenized_train_dataset = train_dataset.map(tokenize_data, fn_kwargs={'tokenizer': tokenizer})\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_data, fn_kwargs={'tokenizer': tokenizer})\n",
    "print(tokenized_test_dataset)\n",
    "print(tokenized_test_dataset['train'][35]['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50879\n",
      "['<unk>', '<eos>', '<Historical fiction>', '<Novel>', '<Alternate history>', '<Gothic fiction>', '<Autobiography>', '<Short story>', '<Biography>', '<Crime Fiction>']\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab_from_iterator(tokenized_train_dataset['train']['tokens'], min_freq=3) \n",
    "vocab.insert_token('<unk>', 0)  # unknown words (if don't know, will return unk)       \n",
    "vocab.insert_token('<eos>', 1)  # learn End of Sentence\n",
    "genres = list(set(train_dataset['train']['Genres']).union(test_dataset['train']['Genres']))\n",
    "for index, genre in enumerate(genres): vocab.insert_token('<{}>'.format(genre), index + 2)           \n",
    "vocab.set_default_index(vocab['<unk>'])   \n",
    "print(len(vocab))                         \n",
    "print(vocab.get_itos()[:10])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []                                                   \n",
    "    for example in dataset:\n",
    "        if example['tokens']:\n",
    "            # prepend\n",
    "            tokens = example['tokens'].insert(0, '<{}>'.format(example['Genres']))\n",
    "            # append                                      \n",
    "            tokens = example['tokens'].append('<eos>')             \n",
    "            tokens = [vocab[token] for token in example['tokens']] \n",
    "            data.extend(tokens)                                    \n",
    "    data = torch.LongTensor(data)                                 \n",
    "    num_batches = data.shape[0] // batch_size \n",
    "    data = data[:num_batches * batch_size]                       \n",
    "    data = data.view(batch_size, num_batches)          \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_data = get_data(tokenized_train_dataset['train'], vocab, batch_size)\n",
    "test_data = get_data(tokenized_test_dataset['train'], vocab, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate):    \n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, \n",
    "                    dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "\n",
    "    def forward(self, src, hidden):\n",
    "        embedding = self.dropout(self.embedding(src))\n",
    "        output, hidden = self.lstm(embedding, hidden)          \n",
    "        output = self.dropout(output) \n",
    "        prediction = self.fc(output)\n",
    "        return prediction, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        return hidden, cell\n",
    "    \n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden\n",
    "        hidden = hidden.detach()\n",
    "        cell = cell.detach()\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 128 \n",
    "hidden_dim = 512             \n",
    "num_layers = 3\n",
    "dropout_rate = 0.2                             \n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 38,130,751 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, num_batches, idx):\n",
    "    src = data[:, idx:idx+seq_len]                   \n",
    "    target = data[:, idx+1:idx+seq_len+1]             \n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    \n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):  # The last batch can't be a src\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        src, target = get_batch(data, seq_len, num_batches, idx)\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        prediction, hidden = model(src, hidden)               \n",
    "\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)   \n",
    "        target = target.reshape(-1)\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "            src, target = get_batch(data, seq_len, num_batches, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            batch_size= src.shape[0]\n",
    "\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "seq_len = 50\n",
    "clip = 0.25\n",
    "saved = False\n",
    "train_ppm = []\n",
    "test_ppm = []\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "\n",
    "if saved:\n",
    "    model.load_state_dict(torch.load('best-val-lstm_lm.pt',  map_location=device))\n",
    "    test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
    "    print(f'Test Perplexity: {math.exp(test_loss):.3f}')\n",
    "else:\n",
    "    best_test_loss = float('inf')\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = train(model, train_data, optimizer, criterion, \n",
    "                    batch_size, seq_len, clip, device)\n",
    "        test_loss = evaluate(model, test_data, criterion, batch_size, \n",
    "                    seq_len, device)\n",
    "        \n",
    "        lr_scheduler.step(test_loss)\n",
    "\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            torch.save(model.state_dict(), 'best-val-lstm_lm.pt')\n",
    "\n",
    "        train_pp = math.exp(train_loss)\n",
    "        test_pp = math.exp(test_loss)\n",
    "\n",
    "        train_ppm.append(train_pp)\n",
    "        test_pp.append(test_pp)\n",
    "\n",
    "        print(f'\\tTrain Perplexity: {train_pp:.3f}')\n",
    "        print(f'\\tTest Perplexity: {test_pp:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n_finalproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f3e6f7a20edffb9d038ab44e54cc16a497f6e240d17c5826cba63a762f9592a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
